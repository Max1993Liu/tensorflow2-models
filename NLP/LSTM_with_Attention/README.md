# RNN with attention mechanism



### Reference  
1. [Keras implementation](https://github.com/philipperemy/keras-attention-mechanism)
2. [Another keras implementation](https://github.com/thushv89/attention_keras)  
3. [Yet another one](https://github.com/ShawnyXiao/TextClassification-Keras/tree/master/model/TextAttBiRNN)
4. [Original paper](https://arxiv.org/abs/1512.08756)  
5. [Pytorch tutorial](https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb)

